import asyncio, sys

# âœ… Windows ç‰¹æ®Šè™•ç†é¸æ“‡ AsyncioSelectorReactor â†’ èˆ‡ Scrapy ç›¸å®¹
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

# âœ… åœ¨ä»»ä½• Twisted import ä¹‹å‰å®‰è£ reactor
import scrapy.utils.reactor
scrapy.utils.reactor.install_reactor("twisted.internet.asyncioreactor.AsyncioSelectorReactor")


import subprocess, logging, time
from pathlib import Path
from scrapy.crawler import CrawlerProcess, CrawlerRunner
from scrapy.utils.project import get_project_settings
from twisted.internet import reactor, defer
from twisted.internet.error import ReactorNotRestartable

from moviescraper.spiders import amba, showTimes, sk, vs, venice
from moviescraper.utils.data_merger import merge_cleaned_outputs

SPIDER_MAP = {
    'amba': amba.AmbassadorSpider,
    'showtimes': showTimes.ShowTimeSpider,
    'sk': sk.skSpider,
    'vs': vs.vsSpider,
    'venice': venice.VeniceSpider,
}

def finish_report(report):
    for name in report:
        end = time.time()
        duration = end - report[name]['start']
        minutes, seconds = divmod(duration, 60)
        report[name]['end'] = end
        report[name]['duration'] = f"{int(minutes)} åˆ† {int(seconds)} ç§’"
    return report

def print_report(report):
    print("\nâœ… æ‰€æœ‰çˆ¬èŸ²å®Œæˆï¼ŒåŸ·è¡Œæ™‚é–“å¦‚ä¸‹ï¼š")
    for name, info in report.items():
        print(f"- {name}: {info['duration']}")


# ------------------------------------------------------------------------------------------
# ğŸ–¥ï¸ CLI æ¨¡å¼ï¼šåŒæ­¥åŸ·è¡Œï¼Œé˜»å¡ä¸»åŸ·è¡Œç·’
# ------------------------------------------------------------------------------------------
def run_all_spiders():
    logging.getLogger('scrapy').setLevel(logging.DEBUG)
    settings = get_project_settings()
    process = CrawlerProcess(settings)

    report = {}
    for name, spider_cls in SPIDER_MAP.items():
        report[name] = {'start': time.time()}
        process.crawl(spider_cls)

    try:
        process.start()
    except ReactorNotRestartable as e:
        print(f"âš ï¸ Reactor ç„¡æ³•é‡å•Ÿï¼š{e}")

    report = finish_report(report)
    print_report(report)

    # ğŸ§© åŸ·è¡Œåˆä½µæ¨¡çµ„ï¼ŒæŠŠæ‰€æœ‰ *_formated.json æ•´åˆæˆ all_cleaned.json
    merge_cleaned_outputs(folder="data", pattern="*_formated.json", output="all_cleaned.json")
    print("[MERGE] âœ… åˆä½µå®Œæˆ â†’ è¼¸å‡º all_cleaned.json")

# -------------------------------------------------------------------------------------------
# ğŸŒ FastAPI webhook / async æ¨¡å¼ï¼šéé˜»å¡ coroutine
# -------------------------------------------------------------------------------------------
@defer.inlineCallbacks
def _crawl_deferred():
    runner = CrawlerRunner(get_project_settings())
    for spider_cls in SPIDER_MAP.values():
        yield runner.crawl(spider_cls)
    reactor.stop()

async def run_safe_spiders():
    print("ğŸŒ å·²åˆ‡æ›ç‚º subprocess æ¨¡å¼ï¼Œè§¸ç™¼ CLI runnerï¼ˆé¿å… Twisted reactor.signal éŒ¯èª¤ï¼‰")
    try:
        spider_path = Path(__file__)
        result = await asyncio.to_thread(
            subprocess.run,
            ["python", str(spider_path), "--cli", "--source=webhook"]
        )
    except Exception as e:
        print(f"âŒ subprocess åŸ·è¡Œå¤±æ•—ï¼Œä¾‹å¤–: {e}")
        return

    if result.returncode != 0:
        print(f"âš ï¸ subprocess åŸ·è¡ŒçµæŸï¼Œä½† returncode é 0ï¼š{result.returncode}")
    else:
        print("âœ… subprocess åŸ·è¡ŒæˆåŠŸï¼Œçˆ¬èŸ²å·²å®Œæˆ")

    # ğŸ§© åŸ·è¡Œåˆä½µæ¨¡çµ„ï¼ŒæŠŠæ‰€æœ‰ *_formated.json æ•´åˆæˆ all_cleaned.json
    merge_cleaned_outputs(folder="data", pattern="*_formated.json", output="all_cleaned.json")
    print("[MERGE] âœ… åˆä½µå®Œæˆ â†’ è¼¸å‡º all_cleaned.json")

# -------------------------------------------------------------------------------------------
# ğŸ§  è‡ªå‹•åµæ¸¬å…¥å£é»
# -------------------------------------------------------------------------------------------
def auto_run_spiders():
    if "--cli" in sys.argv:
        # ç›´æ¥ CLI åŸ·è¡Œ
        print("ğŸ–¥ï¸ åµæ¸¬åˆ° CLI æ¨¡å¼ â†’ åŸ·è¡Œ run_all_spiders()")
        run_all_spiders()
    else:
        # è¢« FastAPI webhook æˆ– task queue å‘¼å«
        print("ğŸŒ åµæ¸¬åˆ°éåŒæ­¥æ¨¡å¼ â†’ åŸ·è¡Œ run_safe_spiders()")
        return run_safe_spiders()


if __name__ == "__main__":
    auto_run_spiders()
