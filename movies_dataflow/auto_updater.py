import os, json, requests, shutil, asyncio, sys, argparse
from spider_executor import SpiderExecutor
from dotenv import load_dotenv
from moviescraper.utils.data_merger import merge_cleaned_outputs

# âœ… Windows asyncio reactor ç›¸å®¹æ€§è™•ç†
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

# âœ… ç’°å¢ƒè®Šæ•¸è¼‰å…¥
load_dotenv()
BASE_URL = os.getenv("BASE_URL", "http://localhost:8000")
UPLOAD_URL = f"{BASE_URL}/upload"
CREDENTIALS_PATH = os.getenv("CREDENTIALS_PATH", "/etc/secrets/credentials.json")
SPREADSHEET_NAME = os.getenv("SPREADSHEET_NAME")

# âœ… æ¸…é™¤ä¸¦å»ºç«‹ data è³‡æ–™å¤¾
def clean_data_folder():
    if os.path.exists("data"):
        shutil.rmtree("data")
    os.makedirs("data")

# âœ… ä¸Šå‚³è³‡æ–™è‡³ FastAPI
def upload_to_fastapi(json_path="data/all_cleaned.json"):
    try:
        with open(json_path, encoding="utf-8") as f:
            payload = json.load(f)
        res = requests.post(UPLOAD_URL, json=payload)
        res.raise_for_status()
        try:
            result = res.json()
        except Exception:
            print("âš ï¸ FastAPI å›å‚³é JSONï¼ŒåŸå§‹å…§å®¹ï¼š", res.text)
            result = {"status": "error", "message": res.text.strip()}
        print(f'âœ… å‚³é€æˆåŠŸï¼š{res.status_code} / å…± {len(payload)} ç­† â†’ {result}')
    except Exception as e:
        print(f'âŒ å‚³é€å¤±æ•—ï¼š{e}')


# âœ… ä¸»åŸ·è¡Œæµç¨‹
def main():
    parser = argparse.ArgumentParser(description="åŸ·è¡Œ Scrapy ä¸¦ä¸Šå‚³çµæœ")
    parser.add_argument("--mode", default="subprocess", choices=["cli", "async", "subprocess"], help="åŸ·è¡Œæ¨¡å¼")
    parser.add_argument("--targets", type=str, help="æŒ‡å®šçˆ¬èŸ²åç¨±ï¼ˆç”¨é€—è™Ÿåˆ†éš”ï¼‰")
    parser.add_argument("--no-upload", action="store_true", help="è·³éä¸Šå‚³æ­¥é©Ÿ")
    parser.add_argument("--dry-run", action="store_true", help="åƒ…åŸ·è¡Œçˆ¬èŸ²ï¼Œä¸åˆä½µã€ä¸ä¸Šå‚³")
    args = parser.parse_args()

    spiders = args.targets.split(",") if args.targets else None

    print(f"ç›®å‰é€²åº¦: æ¸…é™¤ data è³‡æ–™å¤¾")
    clean_data_folder()

    print(f"ç›®å‰é€²åº¦: å•Ÿå‹• Scrapy â†’ æ¨¡å¼: {args.mode} / {spiders or 'å…¨éƒ¨'}")
    SpiderExecutor().run(mode=args.mode, spiders=spiders)

    if args.dry_run:
        print("ğŸ§ª Dry-run æ¨¡å¼ â†’ è·³éåˆä½µèˆ‡ä¸Šå‚³")
        return

    print(f"ç›®å‰é€²åº¦: åˆä½µæ‰€æœ‰å½±åŸè³‡æ–™ â†’ åŒ¯å‡º all_cleaned.json")
    merge_cleaned_outputs("data", "*_formated.json", "all_cleaned.json")

    if args.no_upload:
        print("ğŸ“¦ No-upload æ¨¡å¼ â†’ å·²å®Œæˆåˆä½µï¼Œä½†ä¸åŸ·è¡Œä¸Šå‚³")
        return

    print('ç›®å‰é€²åº¦: å‚³é€è³‡æ–™çµ¦ FastAPI /upload...')
    upload_to_fastapi()


if __name__ == '__main__':
    main()
