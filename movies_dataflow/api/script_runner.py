import threading, time, shutil, datetime, requests, logging, json, os, sys, gspread
from dotenv import load_dotenv
from subprocess import Popen, PIPE
from moviescraper.utils.data_merger import merge_cleaned_outputs
from oauth2client.service_account import ServiceAccountCredentials

# SPIDER_BATCHES = [["amba", "vs"], ["venice", "sk", "showtimes"]]
SPIDER_LIST = ["vs","venice","sk","showtimes","amba", "sbc"]

# âœ… ç’°å¢ƒè®Šæ•¸è¼‰å…¥
load_dotenv()
BASE_URL = os.getenv("BASE_URL", "http://localhost:8000")
UPLOAD_URL = f"{BASE_URL}/upload"
CREDENTIALS_PATH = os.getenv("CREDENTIALS_PATH", "/etc/secrets/credentials.json")
SPREADSHEET_NAME = os.getenv("SPREADSHEET_NAME")

def get_spreadsheet():
    scope = [
        "https://spreadsheets.google.com/feeds",
        "https://www.googleapis.com/auth/drive"
    ]
    creds = ServiceAccountCredentials.from_json_keyfile_name(CREDENTIALS_PATH, scope)
    return gspread.authorize(creds).open(SPREADSHEET_NAME)

def clean_data_folder():
    if os.path.exists("data"):
        shutil.rmtree("data")
    os.makedirs("data")

def run_batch_script_with_ping(script: str, ping_url: str):
    stop_event = threading.Event()
    ping_stats = {"success": 0, "fail": 0}
    start_ts = datetime.datetime.now().isoformat()

    def ping_loop(url: str):
        while not stop_event.is_set():
            try:
                r = requests.get(url, timeout=5)
                ping_stats["success"] += 1
                logging.info(f"ğŸŒ Ping {url} - {r.status_code}")
            except Exception as e:
                ping_stats["fail"] += 1
                logging.warning(f"âš ï¸ Ping failed: {e}")
            time.sleep(120)

    ping_thread = threading.Thread(target=ping_loop, args=(ping_url,))
    ping_thread.start()

    all_stdout, all_stderr = "", ""
    returncode = 0

    try:
        logging.info("ç›®å‰é€²åº¦: æ¸…é™¤ä¸¦å»ºç«‹ data è³‡æ–™å¤¾")
        clean_data_folder()

        for batch in SPIDER_LIST:
            # target_arg = "--targets=" + ",".join(batch)
            proc = Popen([sys.executable, script, "--mode=subprocess", "--targets=" + batch], stdout=PIPE, stderr=PIPE, text=True)
            stdout, stderr = monitor_subprocess(proc, timeout=1800, ping_url=ping_url)

            all_stdout += stdout
            all_stderr += stderr
            logging.info(f"âœ… æ‰¹æ¬¡å®Œæˆ: {batch}")
            logging.info(f"ğŸ“¤ STDOUT:\n{stdout}")
            logging.warning(f"âš ï¸ STDERR:\n{stderr}")

        # æª¢æŸ¥:åœ¨åˆä½µå‰åˆ—å‡ºæ‰€æœ‰æª”æ¡ˆèˆ‡ç­†æ•¸
        logging.info("ğŸ“‹ åˆä½µå‰æª¢æŸ¥ JSON æª”æ¡ˆèˆ‡ç­†æ•¸ï¼š")
        json_summary = []
        for f in os.listdir("data"):
            if f.endswith("_formated.json"):
                path = os.path.join("data", f)
                try:
                    with open(path, encoding="utf-8") as file:
                        items = json.load(file)
                    count = len(items)
                    logging.info(f"ğŸ“¦ {f} â†’ {count} ç­†")
                    json_summary.append(f"{f}:{count}")
                except Exception as e:
                    logging.warning(f"âš ï¸ ç„¡æ³•è®€å– {f}ï¼š{e}")
                    json_summary.append(f"{f}:è®€å–å¤±æ•—")

        # æœ€å¾Œåˆä½µä¸¦ä¸Šå‚³
        for f in os.listdir("data"):
            if f.endswith("_formated.json"):
                logging.info("ç›®å‰é€²åº¦: åˆä½µè³‡æ–™ â†’ åŒ¯å‡º all_cleaned.json")
                merge_cleaned_outputs("data", "*_formated.json", "all_cleaned.json")

                try:
                    with open("data/all_cleaned.json", encoding="utf-8") as f:
                        items = json.load(f)
                    r = requests.post(UPLOAD_URL, json=items)
                    if r.status_code != 200:
                        logging.warning(f"âŒ ä¸Šå‚³å¤±æ•—ï¼š{r.status_code} â†’ {r.text}")
                    else:
                        logging.info(f"âœ… ä¸Šå‚³æˆåŠŸï¼š{r.status_code} â†’ {r.text}")
                except Exception as e:
                    logging.warning(f"âŒ ä¸Šå‚³å¤±æ•—ï¼š{e}")
            else:
                logging.warning("âš ï¸ ç„¡å¯åˆä½µè³‡æ–™ï¼Œè·³éä¸Šå‚³")


    finally:
        stop_event.set()
        ping_thread.join()
        end_ts = datetime.datetime.now().isoformat()
        trace_to_sheet(script, start_ts, end_ts, all_stdout, all_stderr, returncode, ping_stats)

def monitor_subprocess(proc, timeout=1800, ping_url=None):
    start_time = time.time()
    stdout, stderr = "", ""

    while True:
        if proc.poll() is not None:
            break
        elapsed = time.time() - start_time
        if elapsed > timeout:
            proc.kill()
            logging.error("â±ï¸ Subprocess timeoutï¼Œå·²å¼·åˆ¶çµ‚æ­¢")
            break
        if int(elapsed) % 120 == 0 and ping_url:
            try:
                r = requests.get(ping_url, timeout=5)
                logging.info(f"ğŸŒ Ping {ping_url} - {r.status_code}")
            except Exception as e:
                logging.warning(f"âš ï¸ Ping failed: {e}")
        time.sleep(10)

    try:
        stdout, stderr = proc.communicate()
    except Exception as e:
        logging.error(f"âŒ communicate() å¤±æ•—ï¼š{e}")
    return stdout, stderr


def trace_to_sheet(script_name: str, start_ts: str, end_ts: str, stdout: str, stderr: str, returncode: int, ping_stats: dict = None):
    try:
        sheet = get_spreadsheet()
        ws = sheet.worksheet("log")
        row = [
            start_ts,
            end_ts,
            script_name,
            returncode,
            f"âœ… {ping_stats['success']} / âŒ {ping_stats['fail']}" if ping_stats else "",
            stdout[:300],
            stderr[:300]
        ]
        ws.append_row(row)
    except Exception as e:
        logging.error(f"âŒ trace_to_sheet å¯«å…¥å¤±æ•—ï¼š{e}")
